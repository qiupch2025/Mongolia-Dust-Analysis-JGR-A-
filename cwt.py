import os
import numpy as np
import pandas as pd
import xarray as xr
from datetime import datetime
import pytz
import matplotlib.pyplot as plt
import cartopy.io.shapereader as shpreader
from shapely.geometry import Point, box
from shapely.prepared import prep
import warnings

# å¿½ç•¥è­¦å‘Š
warnings.filterwarnings("ignore")

# ==========================================
# 1. å‚æ•°ä¸è·¯å¾„é…ç½®
# ==========================================
# è¯·æ›¿æ¢ä¸ºä½ çš„çœŸå®è·¯å¾„
base_hysplit_path = 'G:/Gç›˜/è’™å¤è’æ¼ åŒ–æ²™å°˜å·¥ä½œ/æ²™å°˜å·¥ä½œ/EST_2/HYSPLIT_DATA/aa_HYSPLIT_NEW/'
pm10_dir = 'G:/Gç›˜/è’™å¤è’æ¼ åŒ–æ²™å°˜å·¥ä½œ/aa.huifu1/airquality/ç«™ç‚¹_20230101-20231007/'
output_dir = 'G:/JGRA_DATA/' 
ndvi_path = r"Z:\Storage(lustre)\ProjectGroup(lzu_public)\lustre_data\EST_2\aaa_new\ndvi\ndvi_monthly_avg_2003-2023.nc"

# å¦‚æœè¾“å‡ºç›®å½•ä¸å­˜åœ¨ï¼Œè‡ªåŠ¨åˆ›å»º
if not os.path.exists(output_dir):
    os.makedirs(output_dir)

city_configs = {
    "Beijing":  "1001A",
    "Lanzhou":  "1476A",
    "Shenyang": "1099A",
    "Taiyuan":  "1081A"
}

# è®¾å®šå¤§ç½‘æ ¼åˆ†è¾¨ç‡ (0.5 åº¦)
res = 0.5
lon_range = [70, 135]
lat_range = [25, 60]

lons_arr = np.arange(lon_range[0], lon_range[1] + res, res, dtype=float)
lats_arr = np.arange(lat_range[0], lat_range[1] + res, res, dtype=float)

# ==========================================
# 2. æ ¸å¿ƒï¼šNDVI æ•°æ®è¯»å–ä¸é‡æ„
# ==========================================
def load_clean_ndvi(path):
    print(f">>> æ­£åœ¨è¯»å– NDVI æ–‡ä»¶: {os.path.basename(path)}")
    ds = xr.open_dataset(path)
    
    # 1. å¯»æ‰¾ç»çº¬åº¦å˜é‡å
    lat_arr = None
    lon_arr = None
    for key in ds.variables:
        if 'lat' in key.lower(): lat_arr = ds[key].values
        if 'lon' in key.lower(): lon_arr = ds[key].values
            
    if lat_arr is None or lon_arr is None:
        raise ValueError("âŒ æ— æ³•åœ¨æ–‡ä»¶ä¸­æ‰¾åˆ°ç»çº¬åº¦å˜é‡ï¼")

    # 2. æå– 2023 æ˜¥å­£æ•°æ® (3,4,5æœˆ)
    # å‡è®¾æ•°æ®ç»´åº¦æ˜¯ (time, lon, lat) æˆ– (time, lat, lon)
    # å‡è®¾æœ€åä¸€å¹´æ˜¯ 2023
    raw_ndvi = ds['ndvi']
    all_vals = raw_ndvi.values 
    
    print("   -> æ­£åœ¨æå– 2023 æ˜¥å­£ (3,4,5æœˆ) å¹³å‡å€¼...")
    
    # æ ¹æ®æ•°æ®ç»´åº¦è¿›è¡Œæå– (è¿™é‡Œå‡è®¾æ˜¯ [month, year, ...])
    # å¦‚æœä½ çš„ nc åªæœ‰ time ç»´ï¼Œé€»è¾‘å¯èƒ½éœ€è¦å¾®è°ƒã€‚è¿™é‡Œæ²¿ç”¨ä¹‹å‰çš„é€»è¾‘ï¼š
    if all_vals.ndim == 4:
        # å– index 2,3,4 (å³ 3,4,5æœˆ), index -1 (æœ€åä¸€å¹´)
        spring_vals = all_vals[2:5, -1, :, :] 
        spring_avg = np.nanmean(spring_vals, axis=0)
    else:
        # é™çº§å¤„ç†
        spring_avg = np.nanmean(all_vals, axis=tuple(range(all_vals.ndim - 2)))

    # 3. è½¬ç½®æ£€æŸ¥ (ç¡®ä¿æ˜¯ Lat x Lon)
    target_shape = (len(lat_arr), len(lon_arr))
    if spring_avg.shape != target_shape:
        print("   -> æ£€æµ‹åˆ°ç»´åº¦è½¬ç½®ï¼Œæ­£åœ¨ä¿®æ­£...")
        spring_avg = spring_avg.T

    # 4. æ„å»º DataArray
    clean_da = xr.DataArray(
        spring_avg,
        coords={'lat': lat_arr, 'lon': lon_arr},
        dims=('lat', 'lon')
    )
    return clean_da.sortby('lat').sortby('lon')

try:
    spring_avg_raw = load_clean_ndvi(ndvi_path)
    print("âœ… NDVI æ•°æ®å‡†å¤‡å°±ç»ªã€‚")
except Exception as e:
    print(f"âŒ NDVI è¯»å–å¤±è´¥: {e}")
    exit()

# ==========================================
# 3. ç”Ÿæˆæ©ç  (æ°´ä½“å‰”é™¤ + é˜ˆå€¼ + æ¹–æ³Šå‰”é™¤)
# ==========================================
n_rows = int(lats_arr.shape[0])
n_cols = int(lons_arr.shape[0])
print(f"   -> ç›®æ ‡ç½‘æ ¼å°ºå¯¸: {n_rows} x {n_cols}")

ndvi_weight_mask = np.zeros((n_rows, n_cols))

# æå–æ•°æ®åŠ é€Ÿ
data_lats = spring_avg_raw.lat.values
data_lons = spring_avg_raw.lon.values
data_vals = spring_avg_raw.values

print("   -> æ­¥éª¤A: è®¡ç®—ç½‘æ ¼ç‰©ç†å±æ€§ (å‰”é™¤åƒç´ çº§æ°´ä½“)...")

for i in range(n_rows):
    for j in range(n_cols):
        lat_s = float(lats_arr[i])
        lon_s = float(lons_arr[j])
        
        # 1. æå–å½“å‰ç½‘æ ¼å†…çš„æ‰€æœ‰ NDVI åƒç´ 
        mask_lat = (data_lats >= lat_s) & (data_lats < lat_s + res)
        mask_lon = (data_lons >= lon_s) & (data_lons < lon_s + res)
        
        valid_rows = data_vals[mask_lat, :]
        valid_pixels = valid_rows[:, mask_lon]
        
        # 2. ã€æ ¸å¿ƒã€‘åƒå…ƒçº§æ°´ä½“æ©è†œï¼šåªä¿ç•™ NDVI >= 0 çš„é™†åœ°åƒç´ 
        valid_land_pixels = valid_pixels[valid_pixels >= 0]
        
        # 3. è®¡ç®—é™†åœ°å¹³å‡å€¼å¹¶åˆ¤æ–­
        if valid_land_pixels.size > 0:
            grid_spatial_mean = np.nanmean(valid_land_pixels)
            if not np.isnan(grid_spatial_mean):
                # é™†åœ°æ¤è¢«ç¨€ç–åº¦åˆ¤æ–­
                if grid_spatial_mean < 0.12: 
                    ndvi_weight_mask[i, j] = 1.0
                else:
                    ndvi_weight_mask[i, j] = 0.0
        else:
            # å…¨æ˜¯æ°´ä½“
            ndvi_weight_mask[i, j] = 0.0

print("   -> æ­¥éª¤B: åº”ç”¨ GIS æ¹–æ³Šæ©è†œ (å‰”é™¤è´åŠ å°”æ¹–ç­‰å¤§å‹æ°´ä½“)...")

# åŠ è½½ Natural Earth æ¹–æ³Šæ•°æ®
reader = shpreader.Reader(shpreader.natural_earth(resolution='50m', category='physical', name='lakes'))
all_lakes = list(reader.geometries())

# ç©ºé—´ç­›é€‰åŠ é€Ÿ (åªä¿ç•™ç ”ç©¶åŒºå†…çš„æ¹–æ³Š)
study_area = box(lon_range[0]-2, lat_range[0]-2, lon_range[1]+2, lat_range[1]+2)
relevant_lakes = [lake for lake in all_lakes if lake.intersects(study_area)]
lake_preps = [prep(lake) for lake in relevant_lakes]

# éå†æ‰€æœ‰è¢«æ ‡è®°ä¸ºæºåŒºçš„ç‚¹ï¼Œæ£€æŸ¥æ˜¯å¦åœ¨æ¹–é‡Œ
rows_idx, cols_idx = np.where(ndvi_weight_mask == 1)
removed_count = 0

for r, c in zip(rows_idx, cols_idx):
    # å–ç½‘æ ¼ä¸­å¿ƒç‚¹åæ ‡
    lat_p = lats_arr[r] + res/2 
    lon_p = lons_arr[c] + res/2
    p = Point(lon_p, lat_p)
    
    for lake in lake_preps:
        if lake.contains(p):
            ndvi_weight_mask[r, c] = 0.0 # å¼ºåˆ¶è®¾ä¸ºéæºåŒº
            removed_count += 1
            break 

print(f"   -> æ©è†œå®Œæˆã€‚å‰”é™¤æ¹–æ³Šè¯¯åˆ¤ç½‘æ ¼æ•°: {removed_count}")

# éªŒè¯æ©è†œæ•ˆæœ
plt.figure(figsize=(8, 5))
plt.imshow(ndvi_weight_mask, origin='lower', 
           extent=[lon_range[0], lon_range[1], lat_range[0], lat_range[1]], 
           cmap='Reds')
plt.colorbar(label='Is Source? (1=Yes)')
plt.title("Final Dust Source Mask")
plt.show()

# ==========================================
# 4. å‡†å¤‡ PM10 æ•°æ®
# ==========================================
def get_pm10_lookup_table(data_dir):
    dates = pd.date_range(start="2023-03-01", end="2023-05-31", freq="D")
    all_data = pd.DataFrame()
    for date in dates:
        fpath = os.path.join(data_dir, f'china_sites_{date.strftime("%Y%m%d")}.csv')
        try:
            df = pd.read_csv(fpath, encoding='utf-8')
            sub = df.iloc[3::15].copy()
            sub.index = pd.date_range(start=date, periods=len(sub), freq='h')
            all_data = pd.concat([all_data, sub])
        except: continue
    if not all_data.empty:
        # ç¡®ä¿æ—¶åŒºä¸º CST (Asia/Shanghai) ä»¥åŒ¹é… HYSPLIT è½¬è¿‡æ¥çš„æ—¶é—´
        all_data.index = all_data.index.tz_localize("Asia/Shanghai") if all_data.index.tz is None else all_data.index
    return all_data

print(">>> æ­£åœ¨åŠ è½½ PM10 æ•°æ®...")
pm10_master = get_pm10_lookup_table(pm10_dir)

# ==========================================
# 5. CWT è®¡ç®— (å«æƒé‡)
# ==========================================
def parse_tdump(file_path):
    pts = []
    if not os.path.exists(file_path): return None
    try:
        with open(file_path, 'r') as f: lines = f.readlines()
        start = 0
        for i, l in enumerate(lines):
            if 'PRESSURE' in l: 
                start = i + 1
                break
        for l in lines[start:]:
            p = l.split()
            if len(p) >= 11: pts.append([float(p[9]), float(p[10])])
    except: return None
    return np.array(pts)

for city, sid in city_configs.items():
    print(f"\n--- å¤„ç†åŸå¸‚: {city} ---")
    traj_dir = os.path.join(base_hysplit_path, city) + '/'
    clus_file = os.path.join(traj_dir, "julei/CLUSLIST_4")
    out_nc = os.path.join(output_dir, f"{city}_PM10_CWT_Analysis_Result.nc")
    
    if not os.path.exists(clus_file): 
        print(f"âš ï¸ æ‰¾ä¸åˆ°èšç±»æ–‡ä»¶: {clus_file}")
        continue

    sum_conc = np.zeros((n_rows, n_cols))
    sum_count = np.zeros((n_rows, n_cols))

    try:
        clus_df = pd.read_csv(clus_file, sep=r'\s+', header=None, engine='python',
                              names=["C", "N", "Y", "M", "D", "H", "I", "Path"])
        
        success_traj = 0
        for _, row in clus_df.iterrows():
            fpath = os.path.join(traj_dir, os.path.basename(str(row['Path']).strip("'")))
            
            # æ—¶é—´è½¬æ¢: UTC -> Local
            dt_utc = datetime(int(row['Y'])+2000, int(row['M']), int(row['D']), int(row['H']))
            dt_loc = pytz.utc.localize(dt_utc).astimezone(pytz.timezone('Asia/Shanghai'))
            
            # PM10 åŒ¹é…
            if dt_loc not in pm10_master.index: continue
            val = pm10_master.loc[dt_loc, sid]
            if pd.isna(val): continue
            
            # è½¨è¿¹è¯»å–
            points = parse_tdump(fpath)
            if points is None: continue
            
            success_traj += 1
            
            # ç½‘æ ¼ç´¯åŠ 
            for lat, lon in points:
                if (lat_range[0] <= lat <= lat_range[1]) and (lon_range[0] <= lon <= lon_range[1]):
                    r_idx = int((lat - lat_range[0]) // res)
                    c_idx = int((lon - lon_range[0]) // res)
                    
                    if 0 <= r_idx < n_rows and 0 <= c_idx < n_cols:
                        sum_conc[r_idx, c_idx] += float(val)
                        sum_count[r_idx, c_idx] += 1
        
        if success_traj > 0:
            # 1. åŸºç¡€ CWT
            cwt_base = np.divide(sum_conc, sum_count, out=np.zeros_like(sum_conc), where=sum_count!=0)
            
            # 2. è®¡ç®—æƒé‡ (Polissar et al.)
            # ä»…ç»Ÿè®¡éé›¶ç½‘æ ¼çš„å¹³å‡ç»è¿‡æ¬¡æ•°
            v_counts = sum_count[sum_count > 0]
            avg = np.mean(v_counts) if len(v_counts) > 0 else 1
            
            w = np.ones_like(sum_count)
            w[sum_count <= 3*avg] = 0.70
            w[sum_count <= 2*avg] = 0.42
            w[sum_count <= avg] = 0.17
            
            cwt_weighted = cwt_base * w
            
            # 3. åº”ç”¨ NDVI ç‰©ç†çº¦æŸ
            cwt_final = cwt_weighted * ndvi_weight_mask 
            
            # 4. ä¿å­˜
            ds = xr.Dataset(
                {
                    "CWT_Final": (["lat", "lon"], cwt_final.astype(np.float32)),
                    "CWT_Original": (["lat", "lon"], cwt_weighted.astype(np.float32)),
                    "Dust_Source_Mask": (["lat", "lon"], ndvi_weight_mask.astype(np.float32)),
                    "Trajectory_Count": (["lat", "lon"], sum_count.astype(np.int32))
                },
                coords={"lat": lats_arr, "lon": lons_arr}
            )
            ds.to_netcdf(out_nc)
            print(f"âœ… {city} è®¡ç®—å®Œæˆï¼Œå·²ä¿å­˜è‡³ {out_nc}")
        else:
            print(f"âš ï¸ {city} æœ‰æ•ˆè½¨è¿¹æ•°ä¸º 0")
            
    except Exception as e:
        print(f"âš ï¸ {city} å¤„ç†è¿‡ç¨‹ä¸­å‡ºé”™: {e}")

print("\n>>> æ‰€æœ‰ä»»åŠ¡å®Œæˆã€‚")

# import os
# import numpy as np
# import xarray as xr
# import matplotlib.pyplot as plt
# import cartopy.crs as ccrs
# import cartopy.feature as cfeature
# import cartopy.io.shapereader as shpreader
# from cartopy.mpl.ticker import LongitudeFormatter, LatitudeFormatter
# from shapely.geometry import Point
# from shapely.prepared import prep
# import warnings

# warnings.filterwarnings("ignore")

# # ==========================================
# # 1. é…ç½®å‚æ•°
# # ==========================================
# input_dir = 'G:/JGRA_DATA/'
# # åŸå¸‚é¡ºåºï¼šåŒ—äº¬ã€æ²ˆé˜³ã€å…°å·
# cities = ["Beijing", "Shenyang", "Lanzhou"] 
# file_suffix = "_PM10_CWT_Analysis_Result.nc"

# extent = [70, 135, 25, 60]
# city_coords = {
#     "Beijing":  (116.40, 39.90),
#     "Shenyang": (123.43, 41.80),
#     "Lanzhou":  (103.83, 36.06)
# }

# # è‰²æ ‡èŒƒå›´
# vmin = 0
# vmax = 200 

# # ==========================================
# # 2. å‡†å¤‡å›½ç•Œæ•°æ®
# # ==========================================
# print(">>> æ­£åœ¨å‡†å¤‡åœ°ç†æ•°æ®...")
# resolution = '110m'
# shpfilename = shpreader.natural_earth(resolution=resolution, category='cultural', name='admin_0_countries')
# reader = shpreader.Reader(shpfilename)
# countries = list(reader.records())

# mongolia_geom, china_geom = None, None
# for c in countries:
#     name = c.attributes['NAME']
#     if name == 'Mongolia': mongolia_geom = c.geometry
#     elif name == 'China': china_geom = c.geometry

# if not mongolia_geom or not china_geom:
#     print("âŒ å›½ç•Œæ•°æ®ç¼ºå¤±")
#     exit()

# mongolia_prep = prep(mongolia_geom)
# china_prep = prep(china_geom)

# def calculate_contribution(ds):
#     """è®¡ç®—è´¡çŒ®å€¼ (åŒ…å«0å€¼ä»¥ä¾¿ç´¯åŠ )"""
#     lats, lons = ds.lat.values, ds.lon.values
#     cwt = ds['CWT_Final'].values
#     cwt = np.nan_to_num(cwt)
    
#     sum_m, sum_c, sum_total = 0, 0, np.sum(cwt)
#     rows, cols = np.where(cwt > 0)
#     for r, c in zip(rows, cols):
#         lat, lon = lats[r], lons[c]
#         p = Point(lon, lat)
#         val = cwt[r, c]
#         if mongolia_prep.contains(p):
#             sum_m += val
#         elif china_prep.contains(p):
#             sum_c += val
            
#     sum_o = sum_total - sum_m - sum_c
#     if sum_o < 0: sum_o = 0
#     return sum_m, sum_c, sum_o, sum_total

# # ==========================================
# # 3. ç»˜å›¾ä¸»ç¨‹åº
# # ==========================================
# def main():
#     print(">>> å¼€å§‹ç»˜åˆ¶ 2x2 å¸ƒå±€å›¾...")
    
#     # ç”»å¸ƒå¤§å° (æ­£æ–¹å½¢å¸ƒå±€ï¼Œç¨å¾®å®½ä¸€ç‚¹)
#     fig = plt.figure(figsize=(14, 12), dpi=300)
    
#     # å­˜å‚¨ç»Ÿè®¡æ•°æ®
#     stats_data = {"Mongolia": [], "China": [], "Others": []}
    
#     # é¢œè‰²è®¾ç½®
#     cmap = plt.cm.get_cmap('jet').copy()
#     cmap.set_bad('white', alpha=0)
    
#     # å®šä¹‰å­å›¾ä½ç½®ç¼–å· (1, 2, 3 æ˜¯åœ°å›¾)
#     # subplot_indices: 
#     # 1 (Top-Left), 2 (Top-Right)
#     # 3 (Bottom-Left), 4 (Bottom-Right -> Bar Chart)
    
#     mesh = None # ç”¨äºè‰²æ ‡
    
#     # --- å¾ªç¯ç»˜åˆ¶ 3 ä¸ªåœ°å›¾ (ä½ç½® 1, 2, 3) ---
#     for i, city in enumerate(cities):
#         # æ³¨æ„ï¼šadd_subplot(2, 2, index) index ä» 1 å¼€å§‹
#         idx = i + 1 
#         ax = fig.add_subplot(2, 2, idx, projection=ccrs.PlateCarree())
        
#         fpath = os.path.join(input_dir, f"{city}{file_suffix}")
#         if not os.path.exists(fpath): continue
        
#         ds = xr.open_dataset(fpath)
        
#         # 1. è®¡ç®—è´¡çŒ®
#         m, c, o, t = calculate_contribution(ds)
#         stats_data["Mongolia"].append(m/t*100)
#         stats_data["China"].append(c/t*100)
#         stats_data["Others"].append(o/t*100)
        
#         # 2. å‡†å¤‡ç»˜å›¾æ•°æ® (æ©è†œ 0 å€¼)
#         plot_cwt = ds['CWT_Final'].where(ds['CWT_Final'] > 0)
        
#         # 3. ç”»çƒ­åŠ›å›¾
#         mesh = plot_cwt.plot.pcolormesh(
#             ax=ax, transform=ccrs.PlateCarree(),
#             cmap=cmap, vmin=vmin, vmax=vmax,
#             add_colorbar=False, rasterized=True
#         )
        
#         # 4. ç”» NDVI < 0.12 çº¢è‰²è½®å»“
#         ax.contour(
#             ds.lon, ds.lat, ds['Dust_Source_Mask'], 
#             levels=[0.5], colors='red', linewidths=0.9, 
#             transform=ccrs.PlateCarree()
#         )
        
#         # 5. åœ°å›¾è£…é¥°
#         ax.set_extent(extent, crs=ccrs.PlateCarree())
#         ax.add_feature(cfeature.LAND, facecolor='white')
#         ax.add_feature(cfeature.COASTLINE, lw=0.6)
#         ax.add_feature(cfeature.BORDERS, linestyle='-', lw=0.4, alpha=0.6)
        
#         # æ ‡è®°åŸå¸‚
#         cx, cy = city_coords[city]
#         ax.plot(cx, cy, marker='*', color='k', ms=14, mec='yellow', mew=0.8, transform=ccrs.PlateCarree(), zorder=10)
        
#         # åæ ‡è½´
#         ax.set_xticks(np.arange(70, 140, 15), crs=ccrs.PlateCarree())
#         ax.set_yticks(np.arange(30, 65, 10), crs=ccrs.PlateCarree())
#         ax.xaxis.set_major_formatter(LongitudeFormatter())
#         ax.yaxis.set_major_formatter(LatitudeFormatter())
#         ax.tick_params(labelsize=10)
        
#         # æ ‡é¢˜ (a) (b) (c)
#         letter = chr(97 + i)
#         ax.set_title(f"({letter}) {city}", loc='left', fontsize=14, fontweight='bold', pad=5)
#         ax.set_xlabel("")
#         ax.set_ylabel("")

#     # --- æ·»åŠ åœ°å›¾ç»Ÿä¸€è‰²æ ‡ (æ°´å¹³ï¼Œæ”¾åœ¨æ•´ä¸ªå›¾çš„åº•éƒ¨ä¸­å¤®) ---
#     # [left, bottom, width, height]
#     cbar_ax = fig.add_axes([0.25, 0.06, 0.5, 0.02]) 
#     cb = fig.colorbar(mesh, cax=cbar_ax, orientation='horizontal', extend='max')
#     cb.set_label('Weighted CWT Concentration ($\mu g/m^3$)', fontsize=12)

#     # ==========================================
#     # 4. å³ä¸‹è§’ï¼šç«–å‘å †å æŸ±çŠ¶å›¾ (æ™®é€šåæ ‡ç³»)
#     # ==========================================
#     ax_bar = fig.add_subplot(2, 2, 4) # ç¬¬ 4 ä¸ªä½ç½®
    
#     # æŸ±çŠ¶å›¾é…è‰² (Nature é£æ ¼)
#     c_m = '#D62728' # è’™å¤ (çº¢)
#     c_c = '#FF7F0E' # ä¸­å›½ (æ©™)
#     c_o = '#F0F0F0' # å…¶ä»– (ç°)
    
#     # Xè½´ä½ç½®
#     x_pos = np.arange(len(cities))
#     bar_width = 0.5
    
#     # ç»˜åˆ¶ç«–å‘å †å æŸ± (Bottom å‚æ•°å…³é”®)
#     # 1. åº•éƒ¨: è’™å¤
#     p1 = ax_bar.bar(x_pos, stats_data["Mongolia"], width=bar_width, color=c_m, label='Mongolia', zorder=3)
    
#     # 2. ä¸­é—´: ä¸­å›½ (bottom=è’™å¤)
#     p2 = ax_bar.bar(x_pos, stats_data["China"], width=bar_width, bottom=stats_data["Mongolia"], 
#                    color=c_c, label='China (Domestic)', zorder=3)
    
#     # 3. é¡¶éƒ¨: å…¶ä»– (bottom=è’™å¤+ä¸­å›½)
#     bottom_others = [m+c for m,c in zip(stats_data["Mongolia"], stats_data["China"])]
#     p3 = ax_bar.bar(x_pos, stats_data["Others"], width=bar_width, bottom=bottom_others, 
#                    color=c_o, edgecolor='gray', label='Others', zorder=3)
    
#     # æ•°å€¼æ ‡ç­¾ (ç«–ç€ç”»æ—¶ï¼Œæ–‡å­—è¦åœ¨æŸ±å­ä¸­é—´)
#     def add_labels(stats, bottom_vals, color='white'):
#         for i, val in enumerate(stats):
#             if val > 5: # æ•°å€¼å¤ªå°ä¸æ˜¾ç¤º
#                 height = bottom_vals[i] + val/2 if bottom_vals else val/2
#                 ax_bar.text(i, height, f"{val:.1f}%", ha='center', va='center', 
#                            color=color, fontweight='bold', fontsize=10)

#     add_labels(stats_data["Mongolia"], None, 'white')
#     add_labels(stats_data["China"], stats_data["Mongolia"], 'white')
#     # add_labels(stats_data["Others"], bottom_others, '#333333') # å…¶ä»–éƒ¨åˆ†é€šå¸¸ä¸éœ€è¦æ ‡ï¼Œé™¤éå¾ˆé‡è¦

#     # ç¾åŒ–æŸ±çŠ¶å›¾
#     ax_bar.set_ylim(0, 100)
#     ax_bar.set_xticks(x_pos)
#     ax_bar.set_xticklabels(cities, fontsize=12, fontweight='bold')
#     ax_bar.set_ylabel('Contribution Percentage (%)', fontsize=12, fontweight='bold')
    
#     # æ ‡é¢˜ (d)
#     ax_bar.set_title("(d) Source Contribution", loc='left', fontsize=14, fontweight='bold')
    
#     # ç½‘æ ¼çº¿ä¸å»è¾¹æ¡†
#     ax_bar.grid(axis='y', linestyle='--', alpha=0.5, zorder=0)
#     ax_bar.spines['top'].set_visible(False)
#     ax_bar.spines['right'].set_visible(False)
    
#     # å›¾ä¾‹ (æ”¾åœ¨å›¾å†…éƒ¨æˆ–ä¸Šæ–¹)
#     ax_bar.legend(loc='upper center', bbox_to_anchor=(0.5, 1.15), ncol=2, frameon=False, fontsize=10)

#     # è°ƒæ•´æ•´ä½“å¸ƒå±€
#     plt.subplots_adjust(wspace=0.15, hspace=0.2, bottom=0.12)

#     # ä¿å­˜
#     save_path = os.path.join(input_dir, "Figure3_2x2_Final.png")
#     plt.savefig(save_path.replace('.png', '.pdf'), bbox_inches='tight')
#     plt.savefig(save_path, bbox_inches='tight', dpi=300)
#     print(f"âœ… 2x2 å¸ƒå±€å›¾å·²ä¿å­˜: {save_path}")
#     plt.show()

# if __name__ == "__main__":
#     main()

# import os
# import numpy as np
# import xarray as xr
# import rioxarray  # æ ¸å¿ƒåº“ï¼šè¿æ¥ xarray å’Œ rasterio
# import geopandas as gpd
# import pandas as pd
# from shapely.geometry import Point, shape
# from rasterio.features import shapes
# import warnings

# warnings.filterwarnings("ignore")

# # ==========================================
# # 1. é…ç½®è·¯å¾„
# # ==========================================
# input_dir = 'G:/JGRA_DATA/' 
# cities = ["Beijing", "Shenyang", "Lanzhou"]
# file_suffix = "_PM10_CWT_Analysis_Result.nc"

# # åŸå¸‚åæ ‡
# city_coords = {
#     "Beijing":  (116.40, 39.90),
#     "Shenyang": (123.43, 41.80),
#     "Lanzhou":  (103.83, 36.06)
# }

# # è¾“å‡ºæ–‡ä»¶å¤¹
# output_gis_dir = os.path.join(input_dir, "GIS_Data_Output")
# if not os.path.exists(output_gis_dir):
#     os.makedirs(output_gis_dir)

# # ==========================================
# # 2. åŠŸèƒ½å‡½æ•°ï¼šå°† xarray è½¬ä¸º GeoTIFF
# # ==========================================
# def export_cwt_to_tiff(city_name):
#     nc_path = os.path.join(input_dir, f"{city_name}{file_suffix}")
#     if not os.path.exists(nc_path):
#         print(f"âš ï¸ æ‰¾ä¸åˆ°æ–‡ä»¶: {nc_path}")
#         return None

#     try:
#         # è¯»å–æ•°æ®
#         ds = xr.open_dataset(nc_path)
        
#         # æå– CWT_Final
#         da = ds['CWT_Final']
        
#         # -------------------------------------------------------
#         # ã€æ ¸å¿ƒä¿®å¤ã€‘ï¼šå¼ºåˆ¶æ”¹åä¸º x, yï¼Œå½»åº•è§£å†³ rioxarray æ‰¾ä¸åˆ°ç»´åº¦çš„é—®é¢˜
#         # -------------------------------------------------------
#         if 'lon' in da.dims and 'lat' in da.dims:
#             da = da.rename({'lon': 'x', 'lat': 'y'})
        
#         # èµ‹äºˆåœ°ç†å‚è€ƒ (WGS84)
#         da.rio.write_crs("EPSG:4326", inplace=True)
        
#         # è®¾ç½® NoData å€¼ (å°† 0 æˆ– NaN è®¾ä¸ºé€æ˜)
#         da = da.where(da > 0) # ç¡®ä¿ 0 å˜æˆ NaN
#         da.rio.write_nodata(np.nan, inplace=True)
        
#         # ä¿å­˜è·¯å¾„
#         tif_path = os.path.join(output_gis_dir, f"{city_name}_CWT_Final.tif")
        
#         # å¯¼å‡º
#         da.rio.to_raster(tif_path, compress='LZW') # LZWå‹ç¼©å‡å°ä½“ç§¯
#         print(f"âœ… [TIFF] {city_name} å·²å¯¼å‡º: {tif_path}")
        
#         return ds # è¿”å› dataset ä¾›ä¸‹ä¸€æ­¥æå–æ©ç ç”¨
        
#     except Exception as e:
#         print(f"âŒ {city_name} TIFF å¯¼å‡ºå¤±è´¥: {e}")
#         return None

# # ==========================================
# # 3. åŠŸèƒ½å‡½æ•°ï¼šå°† Mask è½¬ä¸º Shapefile
# # ==========================================
# def export_mask_to_shp(ds_sample):
#     print(">>> æ­£åœ¨ç”Ÿæˆæ½œåœ¨æºåŒº Shapefile...")
    
#     try:
#         # æå–æ©ç  (0/1 çŸ©é˜µ)
#         mask_da = ds_sample['Dust_Source_Mask']
        
#         # -------------------------------------------------------
#         # ã€æ ¸å¿ƒä¿®å¤ã€‘ï¼šè¿™é‡Œä¹Ÿè¦æ”¹åä¸º x, y æ‰èƒ½æ­£ç¡®è·å– transform
#         # -------------------------------------------------------
#         if 'lon' in mask_da.dims and 'lat' in mask_da.dims:
#             mask_da = mask_da.rename({'lon': 'x', 'lat': 'y'})
        
#         mask_da.rio.write_crs("EPSG:4326", inplace=True)
        
#         # è½¬æ¢ä¸º numpy æ•°ç»„ (å¿…é¡»æ˜¯ float32 æˆ– int)
#         mask_arr = mask_da.values.astype('float32')
        
#         # è·å–ä»¿å°„å˜æ¢å‚æ•° (ç”¨äºå°†æ•°ç»„ç´¢å¼•è½¬ä¸ºç»çº¬åº¦)
#         transform = mask_da.rio.transform()
        
#         # ä½¿ç”¨ rasterio.features.shapes è¿›è¡ŒçŸ¢é‡åŒ–
#         # è¿™ä¸€æ­¥ä¼šæŠŠå€¼ä¸º 1 çš„æ‰€æœ‰ç½‘æ ¼è¿æˆå¤šè¾¹å½¢
#         results = (
#             {'properties': {'value': v}, 'geometry': s}
#             for i, (s, v) in enumerate(shapes(mask_arr, mask=None, transform=transform))
#             if v == 1  # åªä¿ç•™å€¼ä¸º 1 (æºåŒº) çš„éƒ¨åˆ†
#         )
        
#         # æ„å»º GeoDataFrame
#         geoms = list(results)
#         if not geoms:
#             print("âš ï¸ è­¦å‘Šï¼šæ©ç ä¸­æ²¡æœ‰æ£€æµ‹åˆ°æºåŒº (å€¼ä¸º1çš„åŒºåŸŸ)")
#             return

#         polygons = [shape(g['geometry']) for g in geoms]
#         gdf = gpd.GeoDataFrame({'geometry': polygons}, crs="EPSG:4326")
        
#         # ä¿å­˜
#         shp_path = os.path.join(output_gis_dir, "Potential_Dust_Source_Area.shp")
#         gdf.to_file(shp_path, driver='ESRI Shapefile', encoding='utf-8')
#         print(f"âœ… [SHP] æ½œåœ¨æºåŒºçŸ¢é‡å·²å¯¼å‡º: {shp_path}")
        
#     except Exception as e:
#         print(f"âŒ SHP å¯¼å‡ºå¤±è´¥: {e}")

# # ==========================================
# # 4. åŠŸèƒ½å‡½æ•°ï¼šå°†åŸå¸‚ç‚¹ **åˆ†åˆ«** è½¬ä¸º Shapefile (å·²ä¿®æ”¹)
# # ==========================================
# def export_cities_to_shp():
#     print(">>> æ­£åœ¨ç”Ÿæˆå„åŸå¸‚å•ç‹¬çš„ Shapefile...")
    
#     try:
#         for city, (lon, lat) in city_coords.items():
#             # 1. åˆ›å»ºå•ç‚¹æ•°æ®
#             names = [city]
#             geometry = [Point(lon, lat)]
            
#             # 2. åˆ›å»º GeoDataFrame
#             df = pd.DataFrame({'City_Name': names})
#             gdf = gpd.GeoDataFrame(df, geometry=geometry, crs="EPSG:4326")
            
#             # 3. å•ç‹¬ä¿å­˜
#             shp_name = f"{city}_Location.shp"
#             shp_path = os.path.join(output_gis_dir, shp_name)
            
#             gdf.to_file(shp_path, driver='ESRI Shapefile', encoding='utf-8')
#             print(f"âœ… [SHP] {city} ç«™ç‚¹ä½ç½®å·²å¯¼å‡º: {shp_path}")
            
#     except Exception as e:
#         print(f"âŒ åŸå¸‚ç‚¹å¯¼å‡ºå¤±è´¥: {e}")

# # ==========================================
# # 5. ä¸»ç¨‹åº
# # ==========================================
# def main():
#     print(f"STARTING GIS EXPORT -> {output_gis_dir}")
    
#     # 1. å¯¼å‡ºä¸‰ä¸ª TIFF å¹¶è·å–ä¸€ä¸ªæ ·æœ¬ DS (ç”¨äºåš Mask)
#     sample_ds = None
#     for city in cities:
#         ds = export_cwt_to_tiff(city)
#         if sample_ds is None and ds is not None: 
#             sample_ds = ds
            
#     # 2. å¯¼å‡ºæ½œåœ¨æºåŒº SHP (åªéœ€è¦ç”¨å…¶ä¸­ä¸€ä¸ªæ–‡ä»¶çš„ Mask å³å¯ï¼Œå› ä¸ºç‰©ç†çº¦æŸæ˜¯ä¸€æ ·çš„)
#     if sample_ds:
#         export_mask_to_shp(sample_ds)
#     else:
#         print("âš ï¸ æœªèƒ½è¯»å–åˆ°æœ‰æ•ˆçš„ NetCDF æ–‡ä»¶ï¼Œè·³è¿‡ Mask å¯¼å‡ºã€‚")
        
#     # 3. å¯¼å‡ºåŸå¸‚ç‚¹ SHP (åˆ†åˆ«å¯¼å‡º)
#     export_cities_to_shp()
    
#     print("\nğŸ‰ æ‰€æœ‰ GIS æ•°æ®è½¬æ¢å®Œæˆï¼")
#     print(f"æ–‡ä»¶ä½ç½®: {output_gis_dir}")

# if __name__ == "__main__":
#     main()